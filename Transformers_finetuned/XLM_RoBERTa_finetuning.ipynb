{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8feed182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e738bbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, get_scheduler, DataCollatorForTokenClassification\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import evaluate\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a0b13f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8471560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/home/sergey/Python_projects/RU_NER/Project_Data/Data/train_data.csv'\n",
    "test_path = '/home/sergey/Python_projects/RU_NER/Project_Data/Data/test_data.csv'\n",
    "data_files = {'train': train_path, 'test': test_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2782e0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/sergey/.cache/huggingface/datasets/csv/default-f5b328fb0b36accf/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d91fd34b834d45bceec4ad6ed97bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "local_dataset = load_dataset('csv', data_files = data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8acda6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/sergey/.cache/huggingface/datasets/csv/default-f5b328fb0b36accf/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2ef9f8d1ce7e0ca0_*_of_00016.arrow\n",
      "Loading cached processed dataset at /home/sergey/.cache/huggingface/datasets/csv/default-f5b328fb0b36accf/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d81918492de155fe_*_of_00016.arrow\n",
      "Loading cached processed dataset at /home/sergey/.cache/huggingface/datasets/csv/default-f5b328fb0b36accf/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1b8b956640a6ff2a_*_of_00016.arrow\n",
      "Loading cached processed dataset at /home/sergey/.cache/huggingface/datasets/csv/default-f5b328fb0b36accf/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-26420b5be10174f6_*_of_00016.arrow\n"
     ]
    }
   ],
   "source": [
    "local_dataset = local_dataset.map(lambda x: {'augmented_ner_tags': [literal_eval(_) for _ in x['augmented_ner_tags']]}, batched=True, num_proc=16)\n",
    "local_dataset = local_dataset.map(lambda x: {'augmented_tokens': [literal_eval(_) for _ in x['augmented_tokens']]}, batched=True, num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "910d395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dataset = local_dataset.rename_columns({'augmented_tokens': 'tokens',\n",
    "                                             'augmented_ner_tags': 'ner_tags',\n",
    "                                             'sentence': 'sentence'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "831a3693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27d445a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['превращение',\n",
       " 'урана',\n",
       " 'облучённого',\n",
       " 'медленными',\n",
       " 'нейтронами',\n",
       " 'в',\n",
       " 'изотоп',\n",
       " 'бария',\n",
       " 'открыли',\n",
       " 'отто',\n",
       " 'ган',\n",
       " 'и',\n",
       " 'фриц',\n",
       " 'штрассман',\n",
       " 'а',\n",
       " 'теоретическое',\n",
       " 'объяснение',\n",
       " 'открытия',\n",
       " 'сформулировали',\n",
       " 'лиза',\n",
       " 'мейтнер',\n",
       " 'и',\n",
       " 'отто',\n",
       " 'фриш']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sent = local_dataset['train'][10]['tokens']\n",
    "test_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "407ab2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '▁прев',\n",
       " 'ращение',\n",
       " '▁уран',\n",
       " 'а',\n",
       " '▁обл',\n",
       " 'уч',\n",
       " 'ён',\n",
       " 'ного',\n",
       " '▁мед',\n",
       " 'лен',\n",
       " 'ными',\n",
       " '▁ней',\n",
       " 'тро',\n",
       " 'на',\n",
       " 'ми',\n",
       " '▁в',\n",
       " '▁из',\n",
       " 'о',\n",
       " 'топ',\n",
       " '▁бар',\n",
       " 'ия',\n",
       " '▁открыл',\n",
       " 'и',\n",
       " '▁от',\n",
       " 'то',\n",
       " '▁',\n",
       " 'ган',\n",
       " '▁и',\n",
       " '▁фр',\n",
       " 'иц',\n",
       " '▁шт',\n",
       " 'рас',\n",
       " 'сман',\n",
       " '▁а',\n",
       " '▁теоретическ',\n",
       " 'ое',\n",
       " '▁объяс',\n",
       " 'нение',\n",
       " '▁открытия',\n",
       " '▁с',\n",
       " 'форм',\n",
       " 'ул',\n",
       " 'ировали',\n",
       " '▁',\n",
       " 'лиза',\n",
       " '▁ме',\n",
       " 'йт',\n",
       " 'нер',\n",
       " '▁и',\n",
       " '▁от',\n",
       " 'то',\n",
       " '▁фр',\n",
       " 'иш',\n",
       " '</s>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(test_sent, is_split_into_words=True)\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4763b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    return (\n",
    "        local_dataset[\"train\"][i : i + 1000][\"sentence\"]\n",
    "        for i in range(0, len(local_dataset[\"train\"]), 1000)\n",
    "    )\n",
    "\n",
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa5943fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_tokenizer = tokenizer.train_new_from_iterator(training_corpus, vocab_size=128000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43e6f2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250002"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cbed27b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "25b15f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '▁превращение',\n",
       " '▁урана',\n",
       " '▁облуч',\n",
       " 'ён',\n",
       " 'ного',\n",
       " '▁медленным',\n",
       " 'и',\n",
       " '▁нейтрон',\n",
       " 'ами',\n",
       " '▁в',\n",
       " '▁изотоп',\n",
       " '▁бари',\n",
       " 'я',\n",
       " '▁открыли',\n",
       " '▁отто',\n",
       " '▁ган',\n",
       " '▁и',\n",
       " '▁фриц',\n",
       " '▁штрасс',\n",
       " 'ман',\n",
       " '▁',\n",
       " 'а',\n",
       " '▁теоретическо',\n",
       " 'е',\n",
       " '▁объяснение',\n",
       " '▁открытия',\n",
       " '▁сформулировал',\n",
       " 'и',\n",
       " '▁лиза',\n",
       " '▁мейтнер',\n",
       " '▁и',\n",
       " '▁отто',\n",
       " '▁фриш',\n",
       " '</s>']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = new_tokenizer(test_sent, is_split_into_words=True)\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38479cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('XLM_RoBERTA_tokenier/tokenizer_config.json',\n",
       " 'XLM_RoBERTA_tokenier/special_tokens_map.json',\n",
       " 'XLM_RoBERTA_tokenier/tokenizer.json')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new_tokenizer.save_pretrained(\"XLM_RoBERTA_tokenier/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71b4705a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"XLM_RoBERTA_tokenier/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55c636df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a0f7582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2beb513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/sergey/.cache/huggingface/datasets/csv/default-f5b328fb0b36accf/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8f742d2d366f693f_*_of_00016.arrow\n",
      "Loading cached processed dataset at /home/sergey/.cache/huggingface/datasets/csv/default-f5b328fb0b36accf/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-03e3e08134719f07_*_of_00016.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = local_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=local_dataset[\"train\"].column_names,\n",
    "    num_proc=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6be7c71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72454da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84892d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"XLM_RoBERTa_finetuned_with_accelerate/\",\n",
    "                                                        id2label=id2label, label2id=label2id,\n",
    "                                                       num_labels = 9, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73337526",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    "    num_workers=8\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"test\"],\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    "    num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b912371",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, test_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, test_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1d57f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 1\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6f0651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"XLM_RoBERTa_finetuned\"\n",
    "output_dir = \"XLM_RoBERTa_finetuned_with_accelerate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "754ae91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_labels, true_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "053cb460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe08a928c184cdc9e279276291ed3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'precision': 0.6835670900055218, 'recall': 0.6882721197362422, 'f1': 0.6859115363944128, 'accuracy': 0.939897871961087}\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for batch in test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Necessary to pad predictions and labels for being gathered\n",
    "        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(predictions)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "    print(\n",
    "        f\"epoch {epoch}:\",\n",
    "        {\n",
    "            key: results[f\"overall_{key}\"]\n",
    "            for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f4b24a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
